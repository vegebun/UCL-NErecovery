{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22144,"status":"ok","timestamp":1663331923816,"user":{"displayName":"H Li","userId":"11335502756044126854"},"user_tz":-60},"id":"ZlIkiwpay3D0","outputId":"5421584a-e480-4673-db17-b49993d4e267"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Sep 16 12:38:21 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    39W / 300W |   1431MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Baseline_MUSS_corrupt/muss\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/drive/MyDrive/Baseline_MUSS_corrupt/muss\n","Collecting easse@ git+https://github.com/feralvam/easse.git\n","  Cloning https://github.com/feralvam/easse.git to /tmp/pip-install-bg9i0ybm/easse_76042dea87c9417aa7e0c739369a22cf\n","  Running command git clone -q https://github.com/feralvam/easse.git /tmp/pip-install-bg9i0ybm/easse_76042dea87c9417aa7e0c739369a22cf\n","Collecting kenlm@ git+https://github.com/kpu/kenlm.git\n","  Cloning https://github.com/kpu/kenlm.git to /tmp/pip-install-bg9i0ybm/kenlm_cb517a3c178e4d03a4acd403dfdb48bb\n","  Running command git clone -q https://github.com/kpu/kenlm.git /tmp/pip-install-bg9i0ybm/kenlm_cb517a3c178e4d03a4acd403dfdb48bb\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (1.21.6)\n","Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (1.3.5)\n","Requirement already satisfied: nltk>=3.4.3 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (3.7)\n","Requirement already satisfied: tqdm>=4.45.0 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (4.64.1)\n","Requirement already satisfied: sklearn>=0.0 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (0.0)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (1.12.1+cu113)\n","Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (1.7.3)\n","Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (3.1.27)\n","Requirement already satisfied: spacy<3,>=2.1.3 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (2.3.7)\n","Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (1.1.0)\n","Requirement already satisfied: python-Levenshtein>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (0.12.2)\n","Requirement already satisfied: fairseq==0.10.2 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (0.10.2)\n","Requirement already satisfied: truecase in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (0.0.14)\n","Requirement already satisfied: sentencepiece>=0.1.83 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (0.1.97)\n","Requirement already satisfied: imohash>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (1.0.4)\n","Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (4.2.4)\n","Requirement already satisfied: dill>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (0.3.5.1)\n","Requirement already satisfied: submitit in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (1.4.5)\n","Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (1.7.2)\n","Requirement already satisfied: sacremoses>=0.0.38 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (0.0.53)\n","Requirement already satisfied: nevergrad>=0.4.0.post3 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (0.5.0)\n","Requirement already satisfied: editdistance>=0.5.3 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (0.5.3)\n","Requirement already satisfied: tokenizers>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (0.12.1)\n","Requirement already satisfied: cachier>=1.2.8 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (1.5.4)\n","Requirement already satisfied: pyyaml==5.4.1 in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (5.4.1)\n","Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (from muss==1.0) (2.5.1)\n","Collecting tseval@ git+https://github.com/facebookresearch/text-simplification-evaluation.git@main\n","  Cloning https://github.com/facebookresearch/text-simplification-evaluation.git (to revision main) to /tmp/pip-install-bg9i0ybm/tseval_d1df86b1eeec4d6f8e946d2128e10af0\n","  Running command git clone -q https://github.com/facebookresearch/text-simplification-evaluation.git /tmp/pip-install-bg9i0ybm/tseval_d1df86b1eeec4d6f8e946d2128e10af0\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (7.1.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (3.2.2)\n","Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (2.23.0)\n","Requirement already satisfied: sacrebleu>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (2.2.1)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.11.2)\n","Requirement already satisfied: stanfordnlp in /usr/local/lib/python3.7/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.2.0)\n","Requirement already satisfied: yattag in /usr/local/lib/python3.7/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (1.14.0)\n","Requirement already satisfied: plotly>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (5.5.0)\n","Requirement already satisfied: bert_score in /usr/local/lib/python3.7/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.3.11)\n","Requirement already satisfied: simalign in /usr/local/lib/python3.7/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.2->muss==1.0) (2022.6.2)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.2->muss==1.0) (0.6)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.2->muss==1.0) (0.29.32)\n","Requirement already satisfied: hydra-core in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.2->muss==1.0) (1.2.0)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.2->muss==1.0) (1.15.1)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from cachier>=1.2.8->muss==1.0) (0.1.2)\n","Requirement already satisfied: watchdog in /usr/local/lib/python3.7/dist-packages (from cachier>=1.2.8->muss==1.0) (2.1.9)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from cachier>=1.2.8->muss==1.0) (2.5.1)\n","Requirement already satisfied: mmh3>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from imohash>=1.0.4->muss==1.0) (3.0.0)\n","Requirement already satisfied: varint>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from imohash>=1.0.4->muss==1.0) (1.0.2)\n","Requirement already satisfied: cma>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from nevergrad>=0.4.0.post3->muss==1.0) (3.2.2)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from nevergrad>=0.4.0.post3->muss==1.0) (4.1.1)\n","Requirement already satisfied: bayesian-optimization>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from nevergrad>=0.4.0.post3->muss==1.0) (1.2.0)\n","Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization>=1.2.0->nevergrad>=0.4.0.post3->muss==1.0) (1.0.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->muss==1.0) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->muss==1.0) (2.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=4.0.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (1.15.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.0.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (8.0.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein>=0.12.0->muss==1.0) (57.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.21.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.21.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.21.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.21.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (3.0.4)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (4.9.1)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.4.5)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.8.10)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization>=1.2.0->nevergrad>=0.4.0.post3->muss==1.0) (3.1.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (1.1.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (1.0.8)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (7.4.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (2.0.6)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (1.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (1.0.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (3.0.7)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (0.10.1)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (0.7.8)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3,>=2.1.3->muss==1.0) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3,>=2.1.3->muss==1.0) (3.8.1)\n","Requirement already satisfied: transformers>=3.0.0numpy in /usr/local/lib/python3.7/dist-packages (from bert_score->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (4.22.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from bert_score->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->bert_score->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (3.0.9)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.9.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (3.8.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.10.2->muss==1.0) (2.21)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->muss==1.0) (4.0.9)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->muss==1.0) (5.0.0)\n","Requirement already satisfied: omegaconf~=2.2 in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq==0.10.2->muss==1.0) (2.2.3)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq==0.10.2->muss==1.0) (4.9.3)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq==0.10.2->muss==1.0) (5.9.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (1.4.4)\n","Requirement already satisfied: networkx==2.4 in /usr/local/lib/python3.7/dist-packages (from simalign->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (2.4)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx==2.4->simalign->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (4.4.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanfordnlp->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (3.17.3)\n","Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from submitit->muss==1.0) (1.5.0)\n","Installing collected packages: muss\n","  Attempting uninstall: muss\n","    Found existing installation: muss 1.0\n","    Can't uninstall 'muss'. No files were found to uninstall.\n","  Running setup.py develop for muss\n","Successfully installed muss-1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en_core_web_md==2.3.1\n","  Using cached en_core_web_md-2.3.1-py3-none-any.whl\n","Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.3.1) (2.3.7)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (57.4.0)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (7.4.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.7)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.7.8)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.8)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.21.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.23.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.10.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (4.64.1)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (4.12.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.8.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2022.6.15)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_md')\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/Baseline_MUSS_corrupt/muss\n","!pip install -e . \n","# !pip install -U spacy ==2.0.0\n","!python -m spacy download en_core_web_md\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"kusx8vgiz3La","executionInfo":{"status":"ok","timestamp":1663331541615,"user_tz":-60,"elapsed":14309,"user":{"displayName":"H Li","userId":"11335502756044126854"}}},"outputs":[],"source":["\n","from muss.mining.training import get_bart_kwargs, get_score_rows\n","from muss.resources.prepare import prepare_wikilarge_detokenized, prepare_asset\n","from muss.resources.datasets import create_smaller_dataset\n","\n","from muss.fairseq.main import fairseq_train_and_evaluate_with_parametrization\n","from muss.mining.training import get_score_rows #get_bart_kwargs\n","\n","#########################\n","import re\n","import shutil\n","import os \n","\n","from cachier import cachier\n","from easse.cli import evaluate_system_output\n","from easse.utils.constants import TEST_SETS_PATHS\n","import torch\n","from tqdm import tqdm\n","\n","from muss.resources.paths import get_data_filepath, MODELS_DIR, get_dataset_dir\n","from muss.utils.helpers import add_dicts, args_str_to_dict\n","from muss.utils.resources import download_and_extract\n","from muss.preprocessors import GPT2BPEPreprocessor\n","from muss.preprocessing import apply_line_function_to_file\n","from muss.fairseq.main import get_language_from_dataset\n","from muss.text import truncate\n","\n","from muss.simplify import simplify_sentences\n","from muss.utils.helpers import *\n","\n","from muss.simplifiers import get_fairseq_simplifier, get_preprocessed_simplifier\n","from muss.preprocessors import get_preprocessors, get_preprocessor_by_name\n","from muss.mining.training import prepare_bart_model"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"wZqD4xG7NchV","executionInfo":{"status":"ok","timestamp":1663331541615,"user_tz":-60,"elapsed":49,"user":{"displayName":"H Li","userId":"11335502756044126854"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":13,"metadata":{"id":"lHsykczuy_Wx","executionInfo":{"status":"ok","timestamp":1663332252467,"user_tz":-60,"elapsed":1702,"user":{"displayName":"H Li","userId":"11335502756044126854"}}},"outputs":[],"source":["\n","# This dataset should exist in resources/datasets/ and contain the following files:\n","# train.complex, train.simple, valid.complex, valid.simple, test.complex, test.simple\n","def get_evaluate_kwargs(language, phase='valid'):\n","    return {\n","        ('en', 'valid'): {'test_set': 'asset_valid'},\n","        ('en', 'test'): {'test_set': 'asset_test'},\n","        ('fr', 'valid'): {\n","            'test_set': 'custom',\n","            'orig_sents_path': get_data_filepath('alector', 'valid', 'complex'),\n","            'refs_sents_paths': [get_data_filepath('alector', 'valid', 'simple')],\n","        },\n","        ('fr', 'test'): {\n","            'test_set': 'custom',\n","            'orig_sents_path': get_data_filepath('alector', 'test', 'complex'),\n","            'refs_sents_paths': [get_data_filepath('alector', 'test', 'simple')],\n","        },\n","        ('es', 'valid'): {\n","            'test_set': 'custom',\n","            'orig_sents_path': get_data_filepath('simplext_corpus', 'valid', 'complex'),\n","            'refs_sents_paths': [get_data_filepath('simplext_corpus', 'valid', 'simple')],\n","        },\n","        ('es', 'test'): {\n","            'test_set': 'custom',\n","            'orig_sents_path': get_data_filepath('simplext_corpus', 'test', 'complex'),\n","            'refs_sents_paths': [get_data_filepath('simplext_corpus', 'test', 'simple')],\n","        },\n","    }[(language, phase)]\n","\n","evaluate_kwargs=get_evaluate_kwargs('en')\n","\n","\n","def get_predict_files(language):\n","    return {\n","        'en': [get_data_filepath('asset', 'valid', 'complex'), get_data_filepath('asset', 'test', 'complex')],\n","        'fr': [get_data_filepath('alector', 'valid', 'complex'), get_data_filepath('alector', 'test', 'complex')],\n","        'es': [\n","            get_data_filepath('simplext_corpus', 'valid', 'complex'),\n","            get_data_filepath('simplext_corpus', 'test', 'complex'),\n","        ],\n","    }[language]\n","\n","def get_access_preprocessors_kwargs(language, use_short_name=False):\n","    return {\n","        'LengthRatioPreprocessor': {'target_ratio': 0.8, 'use_short_name': use_short_name},\n","        'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.8, 'use_short_name': use_short_name},\n","        'WordRankRatioPreprocessor': {'target_ratio': 0.8, 'language': language, 'use_short_name': use_short_name},\n","        'DependencyTreeDepthRatioPreprocessor': {\n","            'target_ratio': 0.8,\n","            'language': language,\n","            'use_short_name': use_short_name,\n","        },\n","    }\n","\n","def get_bart_kwargs(dataset, language, use_access, use_short_name=False, bart_model='bart.large'):\n","    assert language == 'en'\n","\n","    bart_path = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/resources/models/muss_en_wikilarge_mined/model.pt'   ### Change here\n","    # bart_path = prepare_bart_model(bart_model) / 'model.pt'\n","# \n","    arch = {\n","        'bart.base': 'bart_base',\n","        'bart.large': 'bart_large',\n","        'bart.large.cnn': 'bart_large',\n","    }[bart_model]\n","    kwargs = {\n","        'dataset': dataset,\n","        'metrics_coefs': [0, 1, 0],\n","        'parametrization_budget': 128,\n","        'predict_files': get_predict_files(language), # used in fairseq_evaluate_and_save\n","        'preprocessors_kwargs': {\n","            'GPT2BPEPreprocessor': {},\n","        },\n","        'preprocess_kwargs': {'dict_path': GPT2BPEPreprocessor().dict_path},\n","        'train_kwargs': {\n","            'ngpus': 1,\n","            'arch': arch,\n","            'restore_file': bart_path,\n","            'max_tokens': 4096,\n","            'lr': 8e-05,\n","            'warmup_updates': 500, #500\n","            'truncate_source': True,\n","            'layernorm_embedding': True,\n","            'share_all_embeddings': True,\n","            'share_decoder_input_output_embed': True,\n","            'reset_optimizer': True,\n","            'reset_dataloader': True,\n","            'reset_meters': True,\n","            'required_batch_size_multiple': 1,\n","            'criterion': 'label_smoothed_cross_entropy',\n","            'label_smoothing': 0.1,\n","            'dropout': 0.1,\n","            'attention_dropout': 0.1,\n","            'weight_decay': 0.01,\n","            'optimizer': 'adam',\n","            'adam_betas': '(0.9, 0.999)',\n","            'adam_eps': 1e-08,\n","            'clip_norm': 0.1,\n","            'lr_scheduler': 'polynomial_decay',\n","            'max_update': 10000, # 20000\n","            'skip_invalid_size_inputs_valid_test': True,\n","            'find_unused_parameters': True,\n","        },\n","        'evaluate_kwargs': get_evaluate_kwargs(language),\n","    }\n","    if use_access:\n","        kwargs['preprocessors_kwargs'] = add_dicts(\n","            get_access_preprocessors_kwargs(language, use_short_name=use_short_name), kwargs['preprocessors_kwargs']\n","        )\n","    return kwargs\n","\n","### Compute hardword word retention rate \n","def find_occurence(NE,simple_sentence):\n","  NE_simp = [i.lower() for i in NE['NE_simple']]\n","  NE_diff = [i.lower() for i in NE['NE_difficult']]\n","  simple_sentence = simple_sentence.lower()\n","  intersect_words = []\n","\n","  count_simple = 0\n","  count_difficult = 0\n","  # print(NE_diff)\n","  # print(NE)\n","  if NE_simp[0] != '' or NE_diff:\n","    # print(NE_simp[0] == '')\n","  # print(sentence,test_complex_NER_spacy[index])\n","    if NE_simp[0] != '':\n","      for text in NE_simp:\n","        if text.lower() in simple_sentence:\n","          intersect_words.append(text)\n","          count_simple +=1\n","    else:\n","      count_simple = None\n","    \n","    if NE_diff:\n","      for text in NE_diff:\n","        if text.lower() in simple_sentence:\n","          intersect_words.append(text)\n","          count_difficult +=1\n","          # print('diff words found in sen')\n","    else:\n","      count_difficult = None\n","\n","    return count_simple, count_difficult\n","  else:\n","    # print('empty NE')\n","    return None, None\n","    \n","    \n","def word_retention_rate(comp_sentence,simp_sentence):\n","  total_simp = 0\n","  total_diff = 0\n","  counter_simp = 0\n","  counter_diff = 0\n","\n","  for i,sentence in enumerate(simp_sentence):\n","    NE_temp1 = NE_extraction(comp_sentence[i])[0]\n","\n","    count_simp, count_diff = find_occurence(NE_temp1,sentence)\n","\n","    if count_simp != None or count_diff != None:\n","      if count_simp != None:\n","        total_simp += len(NE_temp1['NE_simple'])\n","        counter_simp += count_simp\n","      if count_diff != None:\n","        total_diff += len(NE_temp1['NE_difficult'])\n","        counter_diff += count_diff\n","    else:\n","      pass\n","    # print(count_simp, count_diff)\n","  if total_diff != 0 and total_simp!=0:\n","    print('-'*50)\n","    print('simple_NE retention rate',counter_simp/total_simp)\n","    print('difficult_NE retention rate',counter_diff/total_diff)\n","    print(total_diff)\n","    print('total_NE retention rate', (counter_simp+counter_diff)/(total_simp+total_diff))\n","    print('-'*50)\n","    return counter_simp/total_simp,counter_diff/total_diff, (counter_simp+counter_diff)/(total_simp+total_diff)\n","\n","  else:\n","    if total_simp != 0:\n","      print('-'*50)\n","      print('simple_NE retention rate',counter_simp/total_simp)\n","      print('-'*50)\n","      return counter_simp/total_simp\n","    if total_diff != 0:\n","      print('-'*50)\n","      print('difficult_NE retention rate',counter_diff/total_diff)\n","      # print('total_NE retention rate', (counter_simp+counter_diff)/(total_simp+total_diff))\n","      print('-'*50)\n","      # print('No stats provided for diff_NE becuase no diff NE at all')\n","    \n","\n","\n","\n","def word_retention_rate_per_sentence(comp_sentence,simp_sentence):\n","  total_simp_list = []\n","  total_diff_list = []\n","  total_list = []\n","  # counter_simp_list = []\n","  # counter_diff_list = []\n","\n","  for i,sentence in enumerate(simp_sentence):\n","    total_simp = 0\n","    total_diff = 0\n","    counter_simp = 0\n","    counter_diff = 0\n","\n","    NE_temp1 = NE_extraction(comp_sentence[i])[0]\n","\n","    count_simp, count_diff = find_occurence(NE_temp1,sentence)\n","    if count_simp != None or count_diff != None:\n","      if count_simp != None:\n","        total_simp += len(NE_temp1['NE_simple'])\n","        counter_simp += count_simp\n","      if count_diff != None:\n","        total_diff += len(NE_temp1['NE_difficult'])\n","        counter_diff += count_diff\n","    else:\n","      pass\n","    # print(count_simp, count_diff)\n","    if total_diff != 0:\n","      # print('simple_NE retention rate',counter_simp/total_simp)\n","      # print('difficult_NE retention rate',counter_diff/total_diff)\n","      # print('total_NE retention rate', (counter_simp+counter_diff)/(total_simp+total_diff))\n","      total_diff_list.append(counter_diff/total_diff)\n","    if total_simp != 0:\n","      total_simp_list.append(counter_simp/total_simp)\n","    if total_simp != 0 or total_diff != 0:\n","      total_list.append((counter_simp+counter_diff)/(total_simp+total_diff))\n","      # if type(counter_diff/total_diff) == float:\n","      #   total_diff_list.append(counter_diff/total_diff)\n","      # else:\n","      #   print(counter_diff,total_diff)\n","      #   total_diff_list.append(counter_diff/total_diff)\n","\n","      # return counter_simp/total_simp,counter_diff/total_diff, (counter_simp+counter_diff)/(total_simp+total_diff)\n","    # else:\n","    #   total_simp_list.append(counter_simp/total_simp)\n","    #   total_list.append((counter_simp+counter_diff)/(total_simp+total_diff))\n","  print('-'*50)\n","  print('Word retention rate per sentence')\n","  print('simple_NE retention rate',np.average(total_simp_list))\n","  print('difficult_NE retention rate',np.average(total_diff_list))\n","  # print(total_diff_list)\n","  print('total_NE retention rate',np.average(total_list))\n","  print('-'*50)\n","\n","\n","def generate_output(chosen_id_list,data_dir_list,data_list,model_dir_dict,sample=False,add_to_dir_name=None,muss_output=False,NE_output=False,CERF_output=False,NE_CERF_output=False,hypothesis_num=None,beam=None,sampling=False):\n","\n","  # store created dir\n","  created_dir = []\n","  output_dir = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/output/'\n","  for index in chosen_id_list:\n","    \n","    # get model unique local id\n","    exp_dir = model_dir_dict[index]['exp_dir']\n","    print('| model_name is',model_dir_dict[index]['model_name'])\n","    print('| exp_dir is',exp_dir)\n","\n","    model_name = 'model_'+ str(index) + '_'+ exp_dir.split('/')[-2]\n","\n","    os.makedirs(output_dir+model_name, exist_ok=True)\n","    print('| output_dir',output_dir)\n","    language = 'en'\n","\n","    # get recommended_preprocessors_kwargs\n","    if 'recommended_preprocessors_kwargs' in model_dir_dict[index].keys() :\n","      preprocessors_kwargs = model_dir_dict[index]['recommended_preprocessors_kwargs']\n","      print('| use provided kwargs')\n","    else:\n","      # use muss_mined_wiki preprocessors\n","      preprocessors_kwargs = {\n","            'LengthRatioPreprocessor': {'target_ratio': 0.9, 'use_short_name': False},\n","            'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.65, 'use_short_name': False},\n","            'WordRankRatioPreprocessor': {'target_ratio': 0.75, 'language': language, 'use_short_name': False},\n","            'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.4, 'language': language, 'use_short_name': False},\n","        }\n","      preprocessors_kwargs['GPT2BPEPreprocessor'] = {}\n","\n","    print('| use preprocessors_kwargs',preprocessors_kwargs)\n","    preprocessors = get_preprocessors(preprocessors_kwargs)\n","\n","    if hypothesis_num == None:\n","      hypothesis_num = 1\n","    if beam == None:\n","      beam=5\n","\n","    generate_kwargs = {'hypothesis_num':hypothesis_num,\n","                       'beam':beam,\n","                       'sampling':sampling}\n","\n","    simplifier = get_fairseq_simplifier(exp_dir, **generate_kwargs)\n","    simplifier = get_preprocessed_simplifier(simplifier, preprocessors=preprocessors)\n","\n","    #########################################\n","    # simplify sample sentences \n","\n","    ############################## I need to change the directory \n","    if sample:\n","      if muss_output:\n","        complex_file_dir = '/content/drive/MyDrive/muss/scripts/contract_no_token.en'\n","      elif NE_output:\n","        complex_file_dir = '/content/drive/MyDrive/muss/scripts/contract_NE_token.en'\n","      elif CERF_output:\n","        complex_file_dir = '/content/drive/MyDrive/muss/scripts/contract_ABCD_token.en'\n","      elif NE_CERF_output:\n","        complex_file_dir = '/content/drive/MyDrive/muss/scripts/contract_NE_ABCD_token.en'\n","\n","      pred_path = simplifier(complex_file_dir)\n","\n","      for i in range(len(read_lines(complex_file_dir))):\n","        print('original:\\n',read_lines(complex_file_dir)[i])\n","        print('simplified:\\n',read_lines(pred_path)[i])\n","        print('----------------------------------------------------')\n","\n","      return read_lines(complex_file_dir),read_lines(pred_path)\n","    ###########################################\n","\n","    # create dir for output\n","    subfolders = os.listdir(output_dir + model_name)\n","    num_subfolder = len(subfolders)\n","    new_folder_dir = output_dir + model_name + '/' +str(num_subfolder).zfill(2) +'/'\n","    os.mkdir(new_folder_dir)\n","    created_dir.append(new_folder_dir)\n","\n","     # simplify asset dataset\n","    for index, file_dir in enumerate(data_dir_list):\n","      \n","      info_file_name = new_folder_dir + 'info.txt'\n","      with open(info_file_name, 'w') as f:\n","        f.write(\"generate_kwargs %s\\n\" % generate_kwargs)\n","      f.close()\n","\n","      filename = new_folder_dir + data_list[index]\n","      print('filename;', filename)\n","      pred_path = simplifier(file_dir)\n","\n","      with open(filename, 'w') as f:\n","          for item in read_lines(pred_path):\n","              f.write(\"%s\\n\" % item)\n","\n","  print('created_dir',created_dir)\n","  return created_dir"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"EqenRaZd21QG","executionInfo":{"status":"ok","timestamp":1663332253697,"user_tz":-60,"elapsed":4,"user":{"displayName":"H Li","userId":"11335502756044126854"}}},"outputs":[],"source":["dataset = 'wikilarge_tag'\n","data_dir = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/resources/datasets/'\n","# %ls '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/resources/datasets/NE_recovery'\n","\n","# read_lines(data_dir+phase+'.'+language)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"3Q0o050Ll8jh","executionInfo":{"status":"ok","timestamp":1663332255706,"user_tz":-60,"elapsed":3,"user":{"displayName":"H Li","userId":"11335502756044126854"}}},"outputs":[],"source":["# read_lines(data_dir+dataset+'/test.simple')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JUJqaX6tzaz8","outputId":"ce51f54c-5a53-49ed-b4e9-301087e5cd11"},"outputs":[{"output_type":"stream","name":"stdout","text":["let the training begin\n","fairseq_prepare_and_train...\n","exp_dir=/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/experiments/fairseq/local_1663332255951\n","preprocess\n","save_interval =  1000\n","fairseq-train /content/drive/MyDrive/Baseline_MUSS_corrupt/muss/resources/datasets/_ed03ee582ca4f7aa06cd126121ac4aa2/fairseq_preprocessed_complex-simple --task translation --source-lang complex --target-lang simple --save-dir /content/drive/MyDrive/Baseline_MUSS_corrupt/muss/experiments/fairseq/local_1663332255951/checkpoints --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --lr-scheduler polynomial_decay --lr 8e-05 --warmup-updates 1000 --update-freq 128 --arch bart_large --dropout 0.1 --weight-decay 0.0 --clip-norm 0.1 --share-all-embeddings --no-epoch-checkpoints --save-interval 999999 --validate-interval 999999 --max-update 4000 --save-interval-updates 1000 --keep-interval-updates 1 --patience 10 --batch-size 64 --seed 341 --distributed-world-size 1 --distributed-port 11926 --fp16 --restore-file '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/resources/models/muss_en_wikilarge_mined/model.pt' --max-tokens 512 --truncate-source --layernorm-embedding --share-all-embeddings --share-decoder-input-output-embed --reset-optimizer --reset-dataloader --reset-meters --required-batch-size-multiple 1 --label-smoothing 0.1 --attention-dropout 0.1 --weight-decay 0.01 --optimizer 'adam' --adam-betas '(0.9, 0.999)' --adam-eps 1e-08 --clip-norm 0.1 --skip-invalid-size-inputs-valid-test --find-unused-parameters\n"]}],"source":["# dataset = 'NE_recovery'\n","kwargs = get_bart_kwargs(dataset=dataset, language='en', use_access=True)\n","kwargs['train_kwargs']['ngpus'] = 1  # Set this from 8 to 1 for local training\n","kwargs['train_kwargs']['max_tokens'] = 512  # Lower this number to prevent OOM\n","kwargs['train_kwargs']['warmup_updates'] = 1000\n","kwargs['train_kwargs']['max_update'] = 4000\n","kwargs['train_kwargs']['save_interval_updates'] = 1000\n","result = fairseq_train_and_evaluate_with_parametrization(**kwargs)\n","print('Training done')"]},{"cell_type":"markdown","metadata":{"id":"wV0VpK57nDt0"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10596796,"status":"ok","timestamp":1662684648378,"user":{"displayName":"H Li","userId":"11335502756044126854"},"user_tz":-60},"id":"GRwggfY63-oB","outputId":"8297fd5f-7892-49a4-d3fb-d34762fa315a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  beams_buf = indices_buf // vocab_size\n","/usr/local/lib/python3.7/dist-packages/fairseq/sequence_generator.py:651: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  unfin_idx = idx // beam_size\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","100%|██████████| 128/128 [2:54:30<00:00, 81.80s/it]"]},{"name":"stdout","output_type":"stream","text":["recommended_preprocessors_kwargs={'LengthRatioPreprocessor': {'target_ratio': 0.7854751500714245, 'use_short_name': False}, 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.7914892289355514, 'use_short_name': False}, 'WordRankRatioPreprocessor': {'target_ratio': 0.8736677657326624, 'language': 'en', 'use_short_name': False}, 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.37999481259333845, 'language': 'en', 'use_short_name': False}, 'GPT2BPEPreprocessor': {}}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["exp_dir ='/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/experiments/fairseq/local_1662592794178/'\n","\n","\n","from muss.fairseq.main import find_best_parametrization\n","recommended_preprocessors_kwargs = find_best_parametrization(exp_dir, **kwargs)\n","print(f'recommended_preprocessors_kwargs={recommended_preprocessors_kwargs}')\n","kwargs['preprocessor_kwargs'] = recommended_preprocessors_kwargs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmmWdesMzjBJ"},"outputs":[],"source":["# print(f'recommended_preprocessors_kwargs={recommended_preprocessors_kwargs}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2kXelwes0jSg"},"outputs":[],"source":["test_simple = read_lines(data_dir+dataset+'/test.simple')\n","test_complex = read_lines(data_dir+dataset+'/test.complex')\n","\n","# word_retention_rate(,test_simple)\n","temp_comp = [NE_extraction(i)[1] for i in test_complex]\n","word_retention_rate(test_complex,temp_comp)\n"]},{"cell_type":"markdown","metadata":{"id":"z4IW7nIZPMa_"},"source":["# Load and evaluate "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xLlyirK0n0S"},"outputs":[],"source":["MODEL_DIR = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/experiments/fairseq/'\n","\n","model_dir_dict = {}\n","def add_item_to_dict(**kwargs):\n","\n","  id = len(model_dir_dict)\n","  model_dir_dict[id]={}\n","\n","  for key, value in kwargs.items():\n","    model_dir_dict[id][key]=value\n","  print('added:',model_dir_dict[id])\n","\n","add_item_to_dict(model_id=len(model_dir_dict),exp_dir= MODEL_DIR+'local_1661468071184/',model_name='NE_recovery_model',recommended_preprocessors_kwargs={'LengthRatioPreprocessor': {'target_ratio': 1.0561701019588032, 'use_short_name': False}, 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.9757367588571789, 'use_short_name': False}, 'WordRankRatioPreprocessor': {'target_ratio': 0.7395441030797956, 'language': 'en', 'use_short_name': False}, 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.5234261949274427, 'language': 'en', 'use_short_name': False}, 'GPT2BPEPreprocessor': {}})\n","test_data_dir_list,test_data_list = ['/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/resources/datasets/NE_recovery/test.complex'],['ne.valid.complex.pred']\n","model_dir_dictx = model_dir_dict\n","\n","model_dir_dictx[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9OQ7t6VYo6b"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okQMov5cP2_q"},"outputs":[],"source":["token_ori_2 = generate_output([0],test_data_dir_list,test_data_list, model_dir_dictx)"]},{"cell_type":"markdown","metadata":{"id":"0gpiECC3ZE9J"},"source":["### Show some result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySUZ-3dxZEwI"},"outputs":[],"source":["comp_sentence, simp_sentence = show_simplify(test_data_dir_list[0],token_ori_2[0]+test_data_list[0])"]},{"cell_type":"markdown","metadata":{"id":"iPvRLcL6i0iF"},"source":["### Analyze recovered entity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZOQhZvhnXpP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ogXNgNUYLEXC"},"outputs":[],"source":["# # test_comp_sen = comp_sentence1[0:2]\n","# word_retention_rate(comp_sentence1,simp_sentence1)\n","# # word_retention_rate_per_sentence(test_comp_sen, [NE_extraction(i)[1] for i in test_comp_sen])\n","# # word_retention_rate(test_comp_sen, [NE_extraction(i)[1] for i in test_comp_sen])\n","\n","# # test_comp_sen\n","# from muss.utils.helpers import *\n","# word_retention_rate(comp_sentence1, simp_sentence1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xARFHZi5sWfp"},"outputs":[],"source":["# def find_num_appered_entity(NER_list,sentence):\n","\n","#   num_appered_entity = 0\n","\n","#   for NER in NER_list:\n","#     if NER in sentence:\n","#       num_appered_entity += 1\n","\n","#   return num_appered_entity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQrpIQ2wsg2k"},"outputs":[],"source":["# find_num_appered_entity([NE_extraction(comp_sentence1[i])[0] for i in range(0,1)]['simple_NE'],simp_sentence1[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AeUuoYuNs8Q2"},"outputs":[],"source":["\n","# [NE_extraction(comp_sentence1(i))[1] for i in range(0,1)]['NE_simple']\n","# [NE_extraction(comp_sentence1[i])[1] for i in range(0,1)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oA6q1ZKfS5UG"},"outputs":[],"source":["\n","\n","test_simple = read_lines(data_dir+dataset+'/test.simple')\n","test_complex = read_lines(data_dir+dataset+'/test.complex')\n","\n","# word_retention_rate(,test_simple)\n","temp_comp = [NE_extraction(i)[1] for i in test_complex]\n","word_retention_rate(comp_sentence,[NE_extraction(i)[1] for i in comp_sentence])\n","print('-'*30)\n","\n","word_retention_rate(comp_sentence, simp_sentence)\n","print(len(simp_sentence))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"paRchokdhuQP"},"outputs":[],"source":["# while True: pass"]},{"cell_type":"markdown","metadata":{"id":"ZrJBsCX01uHw"},"source":["# Full pipeline analysis - Simplify - recovery"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1662500115272,"user":{"displayName":"H Li","userId":"11335502756044126854"},"user_tz":-60},"id":"UFGSiOf8107_","outputId":"a7f97f18-7fb1-491d-80c1-2ff9f9bfc7d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["added: {'model_id': 0, 'exp_dir': '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/experiments/fairseq/local_1660919962534/', 'model_name': 'WIKI_Simplify'}\n"]},{"data":{"text/plain":["{'model_id': 0,\n"," 'exp_dir': '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/experiments/fairseq/local_1660919962534/',\n"," 'model_name': 'WIKI_Simplify'}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["MODEL_DIR = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/experiments/fairseq/'\n","Temp_data_dir = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/qualitative/'\n","Temp_data_name = 'process_sentence1'\n","asset_dir = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/resources/datasets/asset/'\n","asset_ne_dir = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/resources/datasets/asset_ne2/'\n","asset_tag = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/resources/datasets/asset_tag/'\n","# asset_ne_dir = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/resources/datasets/asset_ne2/'\n","model_dir_dict = {}\n","def add_item_to_dict(**kwargs):\n","\n","  id = len(model_dir_dict)\n","  model_dir_dict[id]={}\n","\n","  for key, value in kwargs.items():\n","    model_dir_dict[id][key]=value\n","  print('added:',model_dir_dict[id])\n","\n","add_item_to_dict(model_id=len(model_dir_dict),exp_dir= MODEL_DIR+'local_1660919962534/',model_name='WIKI_Simplify')\n","# test_data_dir_list,test_data_list = ['/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/resources/datasets/0913_ABCD_NER_wikilarge/test.complex'],['test.diff.complex.pred']\n","test_data_dir_list,test_data_list = [asset_tag+'valid.complex'],['test.diff.complex.pred']\n","\n","model_dir_dictx = model_dir_dict\n","model_dir_dictx[0]\n","# read_lines(test_data_dir_list[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQHk4itd144D"},"outputs":[],"source":["token_ori_1 = generate_output([0],test_data_dir_list,test_data_list, model_dir_dictx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8CT3k8Jq5iit"},"outputs":[],"source":["comp_sentence1, simp_sentence1 = show_simplify(test_data_dir_list[0],token_ori_1[0]+test_data_list[0])"]},{"cell_type":"markdown","metadata":{"id":"xM2NNmT_nMoj"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ki7k8UFqSGKn"},"outputs":[],"source":["word_retention_rate(comp_sentence1, simp_sentence1)\n","word_retention_rate_per_sentence(comp_sentence1, simp_sentence1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7B3LvKn_6PVK"},"outputs":[],"source":["def reconstruct_simplified_data(complex_sentence, simple_sentence):\n","    \n","    def list_duplicates_of(seq,item):\n","      start_at = -1\n","      locs = []\n","      while True:\n","          try:\n","              loc = seq.index(item,start_at+1)\n","          except ValueError:\n","              break\n","          else:\n","              locs.append(loc)\n","              start_at = loc\n","      return locs\n","    \n","    control_tokens = [\"<NEXT_NE> \",\"<NEXT_DIFFICULT_WORD>\",\"<SENT_START>\"]\n","\n","\n","#################\n","    indice_of_start = list_duplicates_of(complex_sentence,control_tokens[2])\n","    sentence = complex_sentence[0:indice_of_start[0]] + control_tokens[2] + simple_sentence\n","#################\n","    # Change here to eliminate difficult words \n","    # indice_of_start_diff = list_duplicates_of(complex_sentence,control_tokens[1])\n","    # sentence = complex_sentence[0:indice_of_start_diff[0]] + control_tokens[2] + simple_sentence\n","#################    \n","    return sentence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ztm8qQbWH_e3"},"outputs":[],"source":["# while True: pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZDpTsosB2aq"},"outputs":[],"source":["def reconstruct_simplified_data_extra_NE(complex_sentence, simple_sentence):        ### reconstruct the sentence with only NE that doesn't appear\n","\n","    control_tokens = [\" <NEXT_NE> \",\" <NEXT_DIFFICULT_WORD> \",\" <SENT_START> \"]\n","    # print('-'*10)\n","    temp_NE = NE_extraction(complex_sentence)[0]\n","    new_NE = {'NE_simple':[],'NE_difficult':[]}\n","    # print(temp_NE)\n","    for key, value in temp_NE.items():\n","      for word in temp_NE[key]:\n","        if word != '' and word != ['']:\n","          if word not in simple_sentence:\n","            new_NE[key].append(word)\n","        else:\n","          # print(word)\n","          pass\n","    # print(new_NE,'---')\n","\n","#################\n","    # indice_of_start = list_duplicates_of(complex_sentence,control_tokens[2])\n","    sentence = control_tokens[0] + control_tokens[0].join(new_NE['NE_simple']) + control_tokens[2] + simple_sentence\n","#################\n","    # Change here to eliminate difficult words \n","    # indice_of_start_diff = list_duplicates_of(complex_sentence,control_tokens[1])\n","\n","    # sentence = control_tokens[0] + control_tokens[0].join(new_NE['NE_simple']) + control_tokens[1].join(new_NE['NE_difficult']) + control_tokens[2] + simple_sentence\n","#################    \n","    return sentence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Yxm3y6i-0wi"},"outputs":[],"source":["### Find the sentence that dont have any prefix existed\n","\n","\n","comp_sentence1_interstage = []\n","sentence_no_retention_index = [] ###\n","for i, sen in enumerate(comp_sentence1):\n","  sentence = reconstruct_simplified_data_extra_NE(sen,simp_sentence1[i])\n","  tempNE = NE_extraction(sentence)[0]\n","\n","  var = 1\n","  tempNE_list = []\n","  for key,value in tempNE.items():\n","    # print(value)\n","    for k in value:\n","      tempNE_list.append(k)\n","  # print(tempNE_list)\n","  if tempNE_list[0] != '':\n","    var = 0   ### Since the NE list is not None \n","\n","  if var == 1:\n","    comp_sentence1_interstage.append(sen)\n","  else:\n","    comp_sentence1_interstage.append(sentence)\n","    sentence_no_retention_index.append(i)\n","\n","  # print('-----')\n","\n","print(comp_sentence1_interstage[:10])\n","len(sentence_no_retention_index)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MK4IENwQuZAf"},"outputs":[],"source":["# for i in range(0,5):\n","#   print(comp_sentence1[i],'\\n', simp_sentence1[i])\n","#   print(reconstruct_simplified_data_extra_NE(comp_sentence1[i], simp_sentence1[i]))\n","#   print('-')\n","zeropercent_retention_sentence = [comp_sentence1_interstage[i] for i in sentence_no_retention_index]\n","zeropercent_retention_sentence[0:10]\n","# word_retention_rate(zeropercent_retention_sentence,[NE_extraction(i)[1] for i in zeropercent_retention_sentence]) \n","# word_retention_rate(comp_sentence1_interstage,[NE_extraction(i)[1] for i in comp_sentence1_interstage]) \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TG4uOadIVAau"},"outputs":[],"source":["\n","word_retention_rate(zeropercent_retention_sentence,[NE_extraction(i)[1] for i in zeropercent_retention_sentence]) \n","print(len(zeropercent_retention_sentence))\n","print('NO NE has been RETAINED')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"enUkvJYuJ9Id"},"outputs":[],"source":["with open(Temp_data_dir+Temp_data_name,'w') as f:\n","  # print(type(sentence))\n","  for sen in zeropercent_retention_sentence:\n","    f.write(\"{}\\n\".format(sen))\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48t1aXsxQ_a9"},"outputs":[],"source":["comp_sentence = show_simplify(Temp_data_dir+Temp_data_name)\n","len(comp_sentence)\n","# read_lines(comp_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFQ52kEDE52x"},"outputs":[],"source":["model_dir_dict = {}\n","add_item_to_dict(model_id=len(model_dir_dict),exp_dir= MODEL_DIR+'local_1661468071184/',model_name='NE_recovery')\n","#recommended_preprocessors_kwargs={'LengthRatioPreprocessor': {'target_ratio': 1.0561701019588032, 'use_short_name': False}, 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.9757367588571789, 'use_short_name': False}, 'WordRankRatioPreprocessor': {'target_ratio': 0.7395441030797956, 'language': 'en', 'use_short_name': False}, 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.5234261949274427, 'language': 'en', 'use_short_name': False}, 'GPT2BPEPreprocessor': {}}\n","test_data_dir_list,test_data_list = [Temp_data_dir+Temp_data_name],['valid.diff.complex.pred.1']\n","model_dir_dictx = model_dir_dict\n","model_dir_dictx[0]\n","comp_sentence = show_simplify(test_data_dir_list[0])\n","comp_sentence[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdG2CuGoFbX0"},"outputs":[],"source":["token_ori_2 = generate_output([0],test_data_dir_list,test_data_list, model_dir_dictx)\n","comp_sentence2, simp_sentence2 = show_simplify(test_data_dir_list[0],token_ori_2[0]+test_data_list[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPSW9EMB8yEc"},"outputs":[],"source":["word_retention_rate(comp_sentence2, simp_sentence2)\n","word_retention_rate_per_sentence(comp_sentence2, simp_sentence2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PEofE2dG81-d"},"outputs":[],"source":["simp_sentence[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kp4Cednn88X_"},"outputs":[],"source":["for i,j in enumerate(simp_sentence1):\n","  # print(j)\n","  # print(simp_sentence2[i])\n","  if j == simp_sentence2[i]:\n","    print(True)"]},{"cell_type":"markdown","metadata":{"id":"GL2rojr58uk-"},"source":["# Asset test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jf05FCwJkU0_"},"outputs":[],"source":["asset_dir = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/resources/datasets/asset/'\n","asset_ne_dir = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/resources/datasets/asset_ne2/'\n","asset_complex = read_lines(asset_dir + 'test.complex')\n","# asset_simple = read_lines(asset_dir + 'test.simple.0')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1662324646225,"user":{"displayName":"H Li","userId":"11335502756044126854"},"user_tz":-60},"id":"rWPgf2fW3qmD","outputId":"791992d7-b351-4329-ec08-28ff135d2517"},"outputs":[{"name":"stdout","output_type":"stream","text":["added: {'model_id': 0, 'exp_dir': '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/experiments/fairseq/local_1662324118266/', 'model_name': 'WIKI_Simplify'}\n"]},{"data":{"text/plain":["2000"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# for i,j in enumerate(asset_complex):\n","#   print(j)\n","#   print(asset_simple[i])\n","\n","MODEL_DIR = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/experiments/fairseq/'\n","Temp_data_dir = '/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/qualitative/'\n","Temp_data_name = 'process_sentence2'\n","model_dir_dict = {}\n","def add_item_to_dict(**kwargs):\n","\n","  id = len(model_dir_dict)\n","  model_dir_dict[id]={}\n","\n","  for key, value in kwargs.items():\n","    model_dir_dict[id][key]=value\n","  print('added:',model_dir_dict[id])\n","\n","add_item_to_dict(model_id=len(model_dir_dict),exp_dir= MODEL_DIR+'local_1662324118266/',model_name='WIKI_Simplify')\n","test_data_dir_list,test_data_list = [asset_ne_dir+'valid.complex'],['test.diff.complex.pred']\n","model_dir_dictx = model_dir_dict\n","model_dir_dictx[0]\n","len(read_lines(test_data_dir_list[0]))"]},{"cell_type":"markdown","metadata":{"id":"Yyk7H-hlNDJ_"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536},"executionInfo":{"elapsed":40164,"status":"error","timestamp":1662324686377,"user":{"displayName":"H Li","userId":"11335502756044126854"},"user_tz":-60},"id":"HrwJVJb-5fpC","outputId":"0a34fa03-4c03-401e-b1fa-3b67232adce8"},"outputs":[{"name":"stdout","output_type":"stream","text":["| model_name is WIKI_Simplify\n","| exp_dir is /content/drive/MyDrive/Baseline_MUSS_corrupt/muss/experiments/fairseq/local_1662324118266/\n","| output_dir /content/drive/MyDrive/Baseline_MUSS_corrupt/muss/output/\n","| use preprocessors_kwargs {'LengthRatioPreprocessor': {'target_ratio': 0.9, 'use_short_name': False}, 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.65, 'use_short_name': False}, 'WordRankRatioPreprocessor': {'target_ratio': 0.75, 'language': 'en', 'use_short_name': False}, 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.4, 'language': 'en', 'use_short_name': False}, 'GPT2BPEPreprocessor': {}}\n","filename; /content/drive/MyDrive/Baseline_MUSS_corrupt/muss/output/model_0_local_1662324118266/01/test.diff.complex.pred\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  beams_buf = indices_buf // vocab_size\n"]},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-4b3b9b776ef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken_ori_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data_dir_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir_dictx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-13dc41ca3c90>\u001b[0m in \u001b[0;36mgenerate_output\u001b[0;34m(chosen_id_list, data_dir_list, data_list, model_dir_dict, sample, add_to_dir_name, muss_output, NE_output, CERF_output, NE_CERF_output, hypothesis_num, beam, sampling)\u001b[0m\n\u001b[1;32m    334\u001b[0m       \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_folder_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filename;'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m       \u001b[0mpred_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimplifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/muss/simplifiers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(complex_filepath, pred_filepath)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpred_filepath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mpred_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_temp_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msimplifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred_filepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/muss/simplifiers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(complex_filepath, pred_filepath)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_pred_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0msimplifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Save prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcomplex_filehash\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_filepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/muss/simplifiers.py\u001b[0m in \u001b[0;36mpreprocessed_simplifier\u001b[0;34m(complex_filepath, pred_filepath)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mpreprocessed_complex_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_temp_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mcomposed_preprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_complex_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mpreprocessed_pred_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimplifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_complex_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mcomposed_preprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_pred_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_filepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomplex_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/muss/simplifiers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(complex_filepath, pred_filepath)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpred_filepath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mpred_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_temp_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msimplifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred_filepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/muss/simplifiers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(complex_filepath, pred_filepath)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_pred_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0msimplifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Save prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcomplex_filehash\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_filepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/muss/simplifiers.py\u001b[0m in \u001b[0;36mfairseq_simplifier\u001b[0;34m(complex_filepath, output_pred_filepath)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mmemoize_simplifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfairseq_simplifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_pred_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mfairseq_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_pred_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfairseq_simplifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/muss/fairseq/base.py\u001b[0m in \u001b[0;36mfairseq_generate\u001b[0;34m(complex_filepath, output_pred_filepath, exp_dir, beam, hypothesis_num, lenpen, diverse_beam_groups, diverse_beam_strength, sampling, max_tokens, source_lang, target_lang, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0msampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     )\n","\u001b[0;32m/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/muss/utils/training.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Baseline_MUSS_corrupt/muss/muss/fairseq/base.py\u001b[0m in \u001b[0;36m_fairseq_generate\u001b[0;34m(complex_filepath, output_pred_filepath, checkpoint_paths, complex_dictionary_path, simple_dictionary_path, beam, hypothesis_num, lenpen, diverse_beam_groups, diverse_beam_strength, sampling, max_tokens, source_lang, target_lang, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mmock_cli_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                     \u001b[0mgenerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcli_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mall_hypotheses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfairseq_parse_all_hypotheses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq_cli/generate.py\u001b[0m in \u001b[0;36mcli_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_generation_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args_and_arch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq_cli/generate.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq_cli/generate.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(args, output_file)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mprefix_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m         \u001b[0mnum_generated_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhypos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq/tasks/fairseq_task.py\u001b[0m in \u001b[0;36minference_step\u001b[0;34m(self, generator, models, sample, prefix_tokens, constraints)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             return generator.generate(\n\u001b[0;32m--> 434\u001b[0;31m                 \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m             )\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq/sequence_generator.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, models, sample, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \"\"\"\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     def _generate(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq/sequence_generator.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, sample, prefix_tokens, constraints, bos_token)\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mencoder_outs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mincremental_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m             )\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq/sequence_generator.py\u001b[0m in \u001b[0;36mforward_decoder\u001b[0;34m(self, tokens, encoder_outs, incremental_states, temperature)\u001b[0m\n\u001b[1;32m    825\u001b[0m                     \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                     \u001b[0mencoder_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m                     \u001b[0mincremental_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mincremental_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m                 )\n\u001b[1;32m    829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, prev_output_tokens, encoder_out, incremental_state, features_only, full_context_alignment, alignment_layer, alignment_heads, src_lengths, return_all_hiddens)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mfull_context_alignment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_context_alignment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0malignment_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malignment_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             \u001b[0malignment_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malignment_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m         )\n\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfeatures_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq/models/transformer.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, prev_output_tokens, encoder_out, incremental_state, full_context_alignment, alignment_layer, alignment_heads)\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0mfull_context_alignment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0malignment_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m             \u001b[0malignment_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         )\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq/models/transformer.py\u001b[0m in \u001b[0;36mextract_features_scriptable\u001b[0;34m(self, prev_output_tokens, encoder_out, incremental_state, full_context_alignment, alignment_layer, alignment_heads)\u001b[0m\n\u001b[1;32m    805\u001b[0m                 \u001b[0mself_attn_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0mneed_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0malignment_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m                 \u001b[0mneed_head_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0malignment_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m             )\n\u001b[1;32m    809\u001b[0m             \u001b[0minner_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq/modules/transformer_layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_out, encoder_padding_mask, incremental_state, prev_self_attn_state, prev_attn_state, self_attn_mask, self_attn_padding_mask, need_attn, need_head_weights)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mincremental_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mincremental_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         )\n\u001b[1;32m    354\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq/modules/multihead_attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights)\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0mprev_key_padding_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"prev_key_padding_mask\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msaved_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 282.00 MiB (GPU 0; 15.78 GiB total capacity; 12.29 GiB already allocated; 106.75 MiB free; 14.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["token_ori_1 = generate_output([0],test_data_dir_list,test_data_list, model_dir_dictx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3W83fCxA5iHu"},"outputs":[],"source":["# comp_sentence1, simp_sentence1 = show_simplify(test_data_dir_list[0],token_ori_1[0]+test_data_list[0])\n","# temp_dir = token_ori_1[0]+test_data_list[0]\n","ast.literal_eval(os.popen(\"easse evaluate -t asset_valid -m 'bleu,sari,fkgl' -q < %s\" %token_ori_1[0]+test_data_list[0]).read())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13238,"status":"ok","timestamp":1662324070333,"user":{"displayName":"H Li","userId":"11335502756044126854"},"user_tz":-60},"id":"G886vqyWr1DQ","outputId":"14711e2b-947d-49e1-ee73-8c9d8b082c72"},"outputs":[{"data":{"text/plain":["{'bleu': 58.187,\n"," 'sari': 44.114,\n"," 'fkgl': 6.638,\n"," 'quality_estimation': {'Compression ratio': 1.179,\n","  'Sentence splits': 1.51,\n","  'Levenshtein similarity': 0.732,\n","  'Exact copies': 0.009,\n","  'Additions proportion': 0.358,\n","  'Deletions proportion': 0.175,\n","  'Lexical complexity score': 7.925}}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["\n","ast.literal_eval(os.popen(\"easse evaluate -t asset_valid -m 'bleu,sari,fkgl' -q < %s\" %temp_dir).read())\n","\n","\n","\n","# for i,j in enumerate(comp_sentence1):\n","#   print(j)\n","#   print(simp_sentence1[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NkjjMwg3xIk"},"outputs":[],"source":["# print('without any simplification')\n","# word_retention_rate(comp_sentence1, [NE_extraction(i)[1] for i in comp_sentence1]) ### Original\n","# word_retention_rate_per_sentence(comp_sentence1, [NE_extraction(i)[1] for i in comp_sentence1])\n","# word_retention_rate(comp_sentence1[6:7], simp_sentence1[6:7])\n","print('modified')\n","word_retention_rate(comp_sentence1, simp_sentence1)\n","# NE_extraction(comp_sentence1[0])\n","# comp_sentence1[6:7], simp_sentence1[-1:]\n","print('last year')\n","# from muss.utils.helpers import *\n","# word_retention_rate(comp_sentence1, simp_sentence1)\n","# while True:pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKe2KRtH3-c5"},"outputs":[],"source":["def reconstruct_simplified_data(complex_sentence, simple_sentence):\n","    \n","    def list_duplicates_of(seq,item):\n","      start_at = -1\n","      locs = []\n","      while True:\n","          try:\n","              loc = seq.index(item,start_at+1)\n","          except ValueError:\n","              break\n","          else:\n","              locs.append(loc)\n","              start_at = loc\n","      return locs\n","    \n","    control_tokens = [\"<NEXT_NE> \",\"<NEXT_DIFFICULT_WORD>\",\"<SENT_START> \"]\n","\n","    temp_NE = NE_extraction(complex_sentence)[0]\n","    # for key, value in temp_NE.items():\n","    if temp_NE['NE_simple'] != '' or temp_NE['NE_difficult'] !=[]:\n","  #################\n","      indice_of_start = list_duplicates_of(complex_sentence,control_tokens[2])\n","      sentence = complex_sentence[0:indice_of_start[0]] + control_tokens[2] + simple_sentence\n","  #################\n","      # Change here to eliminate difficult words \n","      # indice_of_start_diff = list_duplicates_of(complex_sentence,control_tokens[1])\n","      # sentence = complex_sentence[0:indice_of_start_diff[0]] + control_tokens[2] + simple_sentence\n","  #################    \n","      return sentence\n","    else:\n","      print('-'*50)\n","      print('occur sentence with no prefix')\n","      print(complex_sentence)\n","      print('-'*50)\n","      pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vA5rDAYNc2aV"},"outputs":[],"source":["### Sentence that have at least one prefixed word not in the complex sentence \n","sentences = []\n","for i, sen in enumerate(comp_sentence1):\n","  NE = NE_extraction(sen)[0]\n","  var = 0 \n","  for key, values in NE.items():\n","\n","    for word in values:\n","      if word not in simp_sentence1[i]:\n","        var = 1 \n","  if var == 1:\n","    sentence = reconstruct_simplified_data(sen,simp_sentence1[i])\n","    sentences.append(sentence)\n","    # print('one sentences has been added')\n","  else:\n","    pass\n","\n","# print(sentences)\n","print(len(sentences))\n","print(len(comp_sentence1))\n","\n","# NE_extraction(sentences)\n","print(word_retention_rate(sentences,[NE_extraction(i)[1] for i in sentences]))\n","# sentences\n","\n","from muss.utils.helpers import *\n","word_retention_rate(sentences, [NE_extraction(i)[1] for i in sentences])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uodIai5WLyUR"},"outputs":[],"source":["### Find the sentence that dont have any prefix existed\n","comp_sentence1_interstage = []\n","sentence_no_retention_index = [] ###\n","for i, sen in enumerate(comp_sentence1):\n","  # print('complex',sen)\n","  # print('simple',simp_sentence1[i])\n","  sentence = reconstruct_simplified_data_extra_NE(sen,simp_sentence1[i])\n","  # print('reconstruct',sentence)\n","  \n","  tempNE = NE_extraction(sentence)[0]\n","\n","  var = 1\n","  tempNE_list = []\n","  for key,value in tempNE.items():\n","    # print(value)\n","    for k in value:\n","      tempNE_list.append(k)\n","  # print(tempNE_list)\n","  if tempNE_list[0] != '':\n","    var = 0   ### Since the NE list is not None \n","  if var == 1:\n","\n","    comp_sentence1_interstage.append(sen)\n","  else:\n","    comp_sentence1_interstage.append(sentence)\n","    sentence_no_retention_index.append(i)\n","\n","# print(comp_sentence1_interstage[:10])\n","len(sentence_no_retention_index)\n","\n","zeropercent_retention_sentence = [comp_sentence1_interstage[i] for i in sentence_no_retention_index]\n","zeropercent_retention_sentence[0:10]\n","print('zero_retention rate sentence')\n","print(word_retention_rate(zeropercent_retention_sentence,[NE_extraction(i)[1] for i in zeropercent_retention_sentence]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"De6e1-DW8RKV"},"outputs":[],"source":["with open(Temp_data_dir+Temp_data_name,'w') as f:\n","  # print(type(sentence))\n","  for sen in zeropercent_retention_sentence:\n","    f.write(\"{}\\n\".format(sen))\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_RqM-tMq80l3"},"outputs":[],"source":["# read_lines(Temp_data_dir+Temp_data_name)\n","\n","model_dir_dict = {}\n","add_item_to_dict(model_id=len(model_dir_dict),exp_dir= MODEL_DIR+'local_1661558615350/',model_name='NE_recovery',recommended_preprocessors_kwargs={'LengthRatioPreprocessor': {'target_ratio': 1.0561701019588032, 'use_short_name': False}, 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.9757367588571789, 'use_short_name': False}, 'WordRankRatioPreprocessor': {'target_ratio': 0.7395441030797956, 'language': 'en', 'use_short_name': False}, 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.5234261949274427, 'language': 'en', 'use_short_name': False}, 'GPT2BPEPreprocessor': {}}\n",")\n","test_data_dir_list,test_data_list = [Temp_data_dir+Temp_data_name],['valid.diff.complex.pred.2']\n","model_dir_dictx = model_dir_dict\n","model_dir_dictx[0]\n","comp_sentence = show_simplify(test_data_dir_list[0])\n","comp_sentence[:5]\n","\n","token_ori_2 = generate_output([0],test_data_dir_list,test_data_list, model_dir_dictx)\n","comp_sentence2, simp_sentence2 = show_simplify(test_data_dir_list[0],token_ori_2[0]+test_data_list[0])\n","word_retention_rate(comp_sentence2, simp_sentence2)\n","# print(len(comp_sentence2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdyeUV4klUW7"},"outputs":[],"source":["from muss.utils.helpers import *\n","word_retention_rate(comp_sentence2, simp_sentence2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnaySZrA86CZ"},"outputs":[],"source":["for i,j in enumerate(comp_sentence2):\n","  print(NE_extraction(j)[1])\n","  print(simp_sentence2[i])\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-EShJ8F-BTb"},"outputs":[],"source":["model_dir_dict = {}\n","add_item_to_dict(model_id=len(model_dir_dict),exp_dir= MODEL_DIR+'local_1661643900834/',model_name='NE_recovery_random_prefix',recommended_preprocessors_kwargs={'LengthRatioPreprocessor': {'target_ratio': 1.1380568424325566, 'use_short_name': False}, 'ReplaceOnlyLevenshteinPreprocessor': {'target_ratio': 0.9597644335499592, 'use_short_name': False}, 'WordRankRatioPreprocessor': {'target_ratio': 0.6931290714995251, 'language': 'en', 'use_short_name': False}, 'DependencyTreeDepthRatioPreprocessor': {'target_ratio': 0.91979820973814, 'language': 'en', 'use_short_name': False}, 'GPT2BPEPreprocessor': {}}\n",")\n","test_data_dir_list,test_data_list = [Temp_data_dir+Temp_data_name],['valid.diff.complex.pred.3']\n","model_dir_dictx = model_dir_dict\n","model_dir_dictx[0]\n","comp_sentence = show_simplify(test_data_dir_list[0])\n","# comp_sentence[:5]\n","\n","token_ori_3 = generate_output([0],test_data_dir_list,test_data_list, model_dir_dictx)\n","comp_sentence3, simp_sentence3 = show_simplify(test_data_dir_list[0],token_ori_3[0]+test_data_list[0])\n","word_retention_rate(comp_sentence3, simp_sentence3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SphBOB4nA7xX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q7GkPVZNBBIH"},"outputs":[],"source":["for i,j in enumerate(simp_sentence2):\n","  print(simp_sentence3[i])\n","  print(j)\n","  # if simp_sentence3[i] != j:\n","  #   print(False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYnArc6-OSJK"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOhkFPIzw8aadFiBtTkndPD"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}